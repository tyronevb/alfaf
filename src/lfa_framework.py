"""
Implements the AutomatedLFAFramework Class.

An instance if the AutomatedLFAFramework implements the end-to-end cognitive debugging framework. This
framework inputs a log file generated by a system and uses a previously trained model to detect failure events and
other errors that occurred during the system's operation.
"""

__author__ = "tyronevb"
__date__ = "2021"

# import classes implementing the subcomponents
from ..data_miner.src.data_miner import DataMiner
from ..inference_engine.src.inference_engine import InferenceEngine

# other imports
import pandas as pd
import datetime
import torch

# todo: test training and inference modes


class AutomatedLFAFramework(object):
    def __init__(self, data_miner_config: str = None,
                 inference_engine_config: str = None,
                 input_dir: str = None,
                 output_dir: str = None,
                 name: str = "default",
                 device: str = None,
                 mode: str = "train",
                 datastore: str = None,
                 update_datastore: bool = False):
        """
        Initialise AutomatedLFAFramework.

        The AutomatedLFAFramework implements the Cognitive Debugging Framework that performs automated, deep learning
        based Log File Analysis to detect possible anomalous events withing system log files.

        The framework utilises the DataMiner and InferenceEngine classes to implement the Data Miner
        and Inference Engine components respectively.

        Two modes of operation are available:
        Training Mode - A datastore of log keys and log event templates is created and an LSTM network is trained
        to model the system's normal behaviour from the log files.
        Inference Mode - Using a previously generated datastore and trained model, detects anomalous events in
        unseen log files generated by a system and produce a Debugging Report to aid in identifying the cause of failures.

        :param data_miner_config: path to configuration file for the DataMiner
        :param inference_engine_config: path to configuration file for the InferenceEngine
        :param input_dir: path to input directory where log files reside, include trailing /
        :param output_dir: path to output directory where all outputs are to be stores, include trailing /
        :param name: a name for this instance of the AutomatedLFAFramework
        :param device: what compute device to use for training: "cpu" or "cuda" if available
        :param mode: mode of operation: "train" or "infer"
        :param datastore: path to previously created datastore. Required if used in Inference Mode
        :param update_datastore: specify whether the datastore is to be updated
        """

        assert type(data_miner_config) == str, "Configuration file for Data Miner must be provided"
        assert type(inference_engine_config) == str, "Configuration file for Inference Engine must be provided"

        # set working attributes
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.name = name
        self.training_mode = True if mode == "train" else False

        # instantiate DataMiner
        self.data_miner = DataMiner(config_file=data_miner_config,
                                    input_dir=self.input_dir,
                                    output_dir=self.output_dir,
                                    verbose=True)

        # instantiate InferenceEngine for either predicting or training -> defined by config file
        self.inference_engine = InferenceEngine(config_file=inference_engine_config,
                                                name=self.name,
                                                device=device,
                                                verbose=True)

        # attribute to keep a copy of the original parsed log; output from data miner - loaded by the _data_preparation method
        self.df_parsed_log = None

        # store path to datastore
        self.datastore = datastore

        # flag to specify whther datastore shouold be updates
        self.update_datastore = update_datastore

    def train_anomaly_detection_model(self, input_log_file_name: str):
        """
        Train an anomaly detection model to detect anomalous events in system log files.

        :param input_log_file_name: filename of the system log file to be used to train the
        Anomaly Detection Model of the Debugging Framework
        :return: None. Learned model parameters are saved to a file
        """

        # load log file, parse log file, extract features, load into DataLoader iterable for training
        data_loader = self._data_preparation(input_log_file_name=input_log_file_name)

        # train the anomaly detection model
        self.inference_engine.train_model(train_data=data_loader)

    def generate_debugging_report(self, input_log_file_name: str, verbose: bool) -> None:
        """
        Perform deep learning-assisted Log FIle Analysis on a given log file and generate
        a debugging report identifying potential anomalies lines in a provided log file.

        Deep Learning-assisted LFA requires a model previously trained on system log from the same system
        as the log files under test.
        :param input_log_file_name: filename of the system log file to be analysed
        :param verbose: Flag to enable printing of statics and some preliminary debug information
        :return: None. Debug Report is written to a file.
        """
        # todo: test

        # first generate the anomaly detection report using the Inference Engine
        anomaly_detection_report = self._infer_and_detect_anomalies(input_log_file_name=input_log_file_name)

        # count of anomalies
        anomaly_cnt = len(anomaly_detection_report) - anomaly_detection_report["anomaly"].value_counts()[0]

        # translate the content of the Anomaly Detection Report to represent Log Keys (from features)
        translated_ad_report = self._translate_to_log_keys(df_anomaly_detection_report=anomaly_detection_report)

        # generate the Debugging Report using the original, parsed log file and the translated anomaly detection report
        debug_report = self._generate_debug_report(df_parsed_log=self.df_parsed_log,
                                                   df_translated_ad_report=translated_ad_report)

        # create a path name for the debug report
        output_path = "{directory}{name}_{timestamp}_debug_report.csv".format(directory=self.output_dir,
                                                                              name=self.name,
                                                                              timestamp=datetime.datetime.now().strftime("%d-%m-%Y-%Hh%Mm%Ss"))
        # write debug report to csv file
        debug_report.to_csv(output_path)

        # print out some stats if verbose is enabled
        if verbose:
            print("Log File analysed: {log_file}".format(log_file=input_log_file_name))
            print("Number of lines: {num_lines}".format(num_lines=len(self.df_parsed_log)))
            print("Number of potential anomalous lines: {anomaly_count}".format(anomaly_count=anomaly_cnt))

    def _data_preparation(self, input_log_file_name: str) -> torch.DataLoader or pd.DataFrame:
        """
        Prepare the input, raw log file for processing.

        Performs log parsing, feature extraction and data loading for training and inference modes of the
        Cognitive Degnugging Framework.
        :param input_log_file_name: filename of the system log file to be analysed
        :return: feature extracted data ready for input to deep learning model.
        In Training Mode: DataLoader iterable
        In Inference Mode: DataFrame
        """

        # parse logs
        path_to_structured_log, path_to_event_matrix = self.data_miner.parse_logs(log_file=input_log_file_name,
                                                                                  save_parameters=True)

        # load the parsed log into a DataFrame
        df_parsed_log = pd.read_csv(path_to_structured_log)

        # store a local copy of the parsed log file for the class
        self.df_parsed_log = df_parsed_log.copy(deep=True)

        # if inference mode, compare to datastore and replace log keys with that in store
        if not self.training_mode:
            df_parsed_log, df_new_events = self._retrieve_log_keys_from_datastore(datastore=self.datastore)
            self.df_parsed_log = df_parsed_log  # update the class reference of the parsed log file with the version with the retrieved log keys
            # update datastore
            if self.update_datastore:
                self._update_datastore(datastore=self.datastore, df_new_log_events=df_new_events)

        # extract features
        features_dataset = self.inference_engine.get_features(df_parsed_log=df_parsed_log)

        if self.training_mode:
            # create datastore
            self._create_datastore(log_events_and_keys=path_to_event_matrix)

            # for training mode, split into batches and encode as tensors
            data_loader = self.inference_engine.batch_and_load_data_to_tensors(features_dataset=features_dataset)
            return data_loader
        else:
            # for inference mode, return the features dataset
            return features_dataset

    def _create_datastore(self, log_events_and_keys: str) -> None:
        """
        Create a Data Store containing log event templates and corresponding keys.

        The Data Store is created during training of the Framework. It is later used during inference to ensure
        that log messages pertaining to a particular event are consistently matched to the same log event keys
        during both training and inference operations on a given system.

        :param log_events_and_keys: path and filename of event occurrence matrix output from the Data Miner
        :return: None. Data Store is created and saved to a csv file
        """

        # open the log event occurrence matrix
        df_datastore = pd.read_csv(log_events_and_keys)

        # select the columns of interest for the Data Store
        df_datastore = df_datastore[["EventId", "EventTemplate"]]

        # extract filename and path before extension
        output_path = "{directory}{name}_{timestamp}_datastore.csv".format(directory=self.output_dir,
                                                                           name=self.name,
                                                                           timestamp=datetime.datetime.now().strftime(
                                                                              "%d-%m-%Y-%Hh%Mm%Ss"))
        # save datastore to csv file
        df_datastore.to_csv(output_path)

        print("Data Store created: {filename".format(filename=output_path))

    def _retrieve_log_keys_from_datastore(self, datastore: str) -> (pd.DataFrame, pd.DataFrame):
        """
        Retrieve log keys for log events from a previously generated data store.

        Log keys are retrieved from a given datastore for a particular system - generated during training -
        when preparing a log file to be used for inference.

        :param datastore: path to and filename for an existing datastore for the particular system
        :return: DataFrame containing of the parsed log file but with log keys retrieved from the datastore
        """

        # load datastore into a DataFrame
        df_datastore = pd.read_csv(datastore)

        # create a dictionary containing the log key event - log key mapping from the datastore
        datastore_dict = dict(zip(df_datastore.EventTemplate, df_datastore.EventId))

        # list to store events not in the datastore
        new_events = []

        # list to store new dataframe rows
        new_df = []

        # create a copy of the original parsed log file
        df_parsed_log = self.df_parsed_log.copy(deep=True)

        # retrieve log event keys from the datastore and replace in the parsed file
        for idx, row in df_parsed_log.iterrows():
            if row["EventTemplate"] in datastore_dict.keys():
                row["EventId"] = datastore_dict[row["EventTemplate"]]
                new_df.append(row)
            else:
                new_df.append(row)
                new_events.append(row)

        # create new dataframe from the modified rows with the log keys from the datastore
        df_new_parsed_log = pd.DataFrame(new_df)

        # create dataframe of the new log events and log keys not in the datastore, could be used to update datastore
        df_new_events = pd.DataFrame(new_events)

        return df_new_parsed_log, df_new_events

    @staticmethod
    def _prepare_new_events_for_datastore(df_new_log_events: pd.DataFrame) -> pd.DataFrame:
        """
        Prepares new, unseen log events for entry into the datastore.

        Extracts the log event keys and the log event templates from the parsed log data structure.

        :param df_new_log_events: DataFrame containing the new, unseen log events not in the Data Store
        :return: DataFrame of the log event keys and log templates of all unique, new log events
        """

        # create a copy of the DataFrame
        df_new_log_events = df_new_log_events.copy(deep=True)

        # extract the EventId and EventTemplate columns
        df_new_log_events = df_new_log_events[["EventId", "EventTemplate"]]

        # drop duplicates
        df_new_log_events = df_new_log_events.drop_duplicates()

        return df_new_log_events

    def _update_datastore(self, datastore: str, df_new_log_events: pd.DataFrame) -> None:
        """
        Updates the Log Key - Log Event Template Data Store with new log keys and events.

        Updates the existing Data Store with the new log events and log keys found during Framework operations
        on the system's log files. - safe update, creates a new file

        :param datastore: path to Data Store
        :param df_new_log_events: DataFrame of the new log event templates and log keys to be added to the Data Store
        :return: None. A new Data Store file is created
        """

        # load the existing datastore
        df_datastore = pd.readcsv(datastore)

        # prepare the new log events
        df_new_log_events = self._prepare_new_events_for_datastore(df_new_log_events=df_new_log_events)

        # add the new log events (keys and templates) to the datastore
        df_updated_datastore = pd.concat([df_datastore, df_new_log_events], axis=0)
        df_updated_datastore.reset_index(drop=True, inplace=True) # reset the dataframe index

        output_path = "{directory}{name}_{timestamp}_datastore.csv".format(directory=self.output_dir,
                                                                           name=self.name,
                                                                           timestamp=datetime.datetime.now().strftime(
                                                                              "%d-%m-%Y-%Hh%Mm%Ss"))
        # save the updated datastore to a csv file
        df_updated_datastore.to_csv(output_path)

        print("Updated Data Store created: {filename".format(filename=output_path))

    def _infer_and_detect_anomalies(self, input_log_file_name: str) -> pd.DataFrame:
        """
        Perform inference using a previously trained model to detect anomalies in a given log file.

        :param input_log_file_name: filename of the system log file to be analysed
        :return: DataFrame containing an Anomaly Detection Report as generated by the Inference Engine
        """
        # todo: test

        # load log file, parse to a structured dataset and extract features
        features_dataset = self._data_preparation(input_log_file_name=input_log_file_name)

        # use a previously trained model to perform inference and detect anomalies
        anomaly_detection_report = self.inference_engine.infer_and_detect_anomalies(input_dataset=features_dataset)

        return anomaly_detection_report

    def _translate_to_log_keys(self, df_anomaly_detection_report: pd.DataFrame) -> pd.DataFrame:
        """
        Method to translate the generated anomaly detection report to log keys from features.

        Only used during inference.

        :param df_anomaly_detection_report: DataFrame containing the anomaly detection report generated by the Framework
        operating in Inference Mode
        :return: DataFrame containing the anomaly detection report translated to log event keys
        """

        # create the reverse transformation to map features back to log keys - use the transformation from the feature extractor
        reverse_transformation = {value: key for key, value in self.inference_engine.feature_extractor.data_transformation.items()}

        # create list to store the transformed entries
        report_lines = []

        # iterate through the generated anomaly detection report, operating on each entry, apply reverse transformation to all elements
        for idx, row in df_anomaly_detection_report.iterrows():
            tmp = dict()
            tmp["log_key_sequence"] = [reverse_transformation[int(feature_key)] for feature_key in row["input_seq"]]
            tmp["actual_next_key"] = [reverse_transformation[int(actual_key)] for actual_key in row["actual_label"]]
            tmp["candidate_keys"] = [reverse_transformation[int(candidate_key)] for candidate_key in row["candidates"]]
            tmp["anomaly"] = row["anomaly"]
            report_lines.append(tmp)

        # generate a new dataframe with the log key report data
        df_anomaly_detection_report_keys = pd.DataFrame(report_lines)

        return df_anomaly_detection_report_keys

    @staticmethod
    def _generate_debug_report(df_parsed_log: pd.DataFrame, df_translated_ad_report: pd.DataFrame) -> pd.DataFrame:
        """
        Generate the Debugging Report.

        Combines the parsed, structured log file with the Anomaly Detection Report. This results in a line-by-line account
        of which log messages or lines in the log file indicate possible points of failure or error within the
        system.

        :param df_parsed_log: DataFrame containing the structured, parsed log file - output from Data Miner
        :param df_translated_ad_report: DataFrame containing the translated anomaly detection report in which log messages
        are identified by log event keys
        :return: DataFrame containing the Debugging Report - original, parsed log file with an analysis
        of each line in the log file describing whether that log message was expected or not, what was expected, and
        whether that log line is a potential anomaly representing a failure or error within system operation
        """

        # create padding DataFrame for the anomaly detection report since it has fewer entries due to the window function
        padding = [{"log_key_sequence": None, "actual_next_key": None, "candidate_keys": None, "anomaly": None}]
        padding = padding * (len(df_parsed_log) - len(df_translated_ad_report))
        df_padding = pd.DataFrame(padding)

        # pad the anomaly detetection report by prepending the padding DataFrame
        df_padded_translated_ad_report = df_padding.append(df_translated_ad_report, ignore_index=True)

        # concatenate the parsed log and padded anomaly detection report DataFrames along the horizontal axis
        df_debug_report = pd.concat([df_parsed_log, df_padded_translated_ad_report], axis=1)

        return df_debug_report

# end
